[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The book of seahtrue",
    "section": "",
    "text": "Preface\nThis is the R Seahorse data analysis manual using functions from the Seahtrue package. Its purpose is to demonstrate and educate how to use R for Extracellular Flux analysis. A dedicated data analysis pipeline is used that allows for quality control and advanced plotting of the data.\nThe manual is designed using a webr/wasm format, so that you can run R in your browser and do not need to install any R or Rstudio software on your computer."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "The book of seahtrue",
    "section": "Resources",
    "text": "Resources\n\nFree books\nTelling Stories with data\nExcellent very complete overview covering basic R coding, communicating science, and statistics Good chapters for example:\n\nClean and prepare https://tellingstorieswithdata.com/09-clean_and_prepare.html\nappendix 1: R essentials https://tellingstorieswithdata.com/20-r_essentials.html\n\nR for data Science\nThe go-to book when you want to get familiar with R and tidy from Hadley Wickham and others from Posit https://r4ds.hadley.nz/\nNice ones for example:\n\nggplot basics https://r4ds.hadley.nz/data-visualize\nlubridate date and time basics https://r4ds.hadley.nz/datetimes\n\nFunctional programming book\nNice visualizations of data structures and operations\n\nvectors, lists, tibbles https://dcl-prog.stanford.edu/data-structure-basics.html\nmap and purrr https://dcl-prog.stanford.edu/purrr-basics.html\n\nFundamentals of Data Visualization\nEverything you want to know about visualizing data using ggplot, with beautifull plots. Also, all code fully available on github\n\nassociations (scatter plots etc) https://clauswilke.com/dataviz/visualizing-associations.html\nuncertainty (error bars and distirbutions etc) https://clauswilke.com/dataviz/visualizing-uncertainty.html\n\n\n\nWebr/WASM\nwebr REPL - a full complete rstudio-like R environment in the browser https://webr.r-wasm.org/latest/\nWebr/wasm presentation bob rudis - (how I got interested in webr/wasm) - https://www.youtube.com/watch?v=inpwcTUmBDY\nWebr manual https://docs.r-wasm.org/webr/latest/\nWebr on github https://github.com/r-wasm/webr/\n\n\nOther info\nLearn to purrr, Rebecca Barter - https://www.rebeccabarter.com/blog/2019-08-19_purrr\n\n\nMovies:\nPurrr and map functions explained by Hadley - https://www.youtube.com/watch?v=EGAs7zuRutY"
  },
  {
    "objectID": "index.html#r-fun",
    "href": "index.html#r-fun",
    "title": "The book of seahtrue",
    "section": "R fun",
    "text": "R fun\nhttps://twitter.com/rafamoral/status/1571622591219236864?s=20&t=RJWOSe30-8bbDxgLIamRUQ"
  },
  {
    "objectID": "jump.html",
    "href": "jump.html",
    "title": "Jump into the water",
    "section": "",
    "text": "In the section, we jump into the water. We learn how to swim in R code and get ourselves familiar with using R for data handling and plotting. We will use first a dataset from R itself and secondly work with a Seahorse data file. There are exercises, with solutions, as well."
  },
  {
    "objectID": "jump_intro.html",
    "href": "jump_intro.html",
    "title": "1  Ditching point-and-click and diving into R",
    "section": "",
    "text": "Data analysis in biological and medical sciences was (and still is) dominated by the use of point-and-click tools, like Excel, Prism, SPSS etc. The reasons for this are that it is easy, it is visual, and gets you fast to an outcome. These point-and-click tools are convenient to use and their main asset is that you have a canvas or grid in front of you for dragging-dropping, copy-pasting, typing and calculating. For plotting data, you can choose formatting options by clicking on the features you want to change, again on a canvas that is in front of you on the screen. This canvas style of working is likely what is closest to our natural way of getting things done; putting stuff together with your hands and seeing directly what happens to the stuff is quick and actionable.\nThe disadvantages of using point-and-click tools in biological and medical sciences are that point-and-click tools are not traceable and can be prone to mistakes. The sequence of clicks and manipulations that have been made to the data was not recorded, which makes the data wrangling process not traceable and proper version control does not exist. Also, you will always works within the limits of the tools that you are using, or how Bruno Rodrigues, the author of the free book Building reproducible analytical pipelines with R, phrased it “… point and click never allow you to go beyond what vendors think you need.” X-link.\nEnter R! R is possibly the best, easiest and most accessible tool to use for biologist and like-minded scientists. R is part of or adopted more and more in scholarly programs at academic institutes not only for statistical use but also for other aspects of data science. Other tools like Python and Matlab are also widely used and they have similar benefits as R over point-and-click tools. Matlab however is proprietary software that needs high licence fees from institutions to be able to work with it.\nHere you will jump straight to using the tidyverse way of working https://www.tidyverse.org/. A complete (but extensive) overview of R data science can be found at https://r4ds.hadley.nz/. The “R for data science” resource also centers around the tidyverse and the tidy concept of data handling. Often data handling (organizing the data and tidying it to be able to use it in your downstream workflow) is describes as data wrangling. As mentioned in the R for data science book: “Together, tidying and transforming are called wrangling because getting your data in a form that’s natural to work with often feels like a fight!” https://r4ds.hadley.nz/intro. With the tidyverse and some level of experience your fight will become less and less over time.\nSince the R community is huge, there is also an overwhelming number of resources (like books, tutorials, videos, blog posts, stack-exchange content) that all want to teach, educate and inform you about R in one way or the other. Also, there are again collections of R resources, and even collections of collections of R resources….\nThese tutorials and courses have one thing in common, they start of with installing R and Rstudio and learning the software. How nice would it be if we can skip these (often) nasty installations? What if we can skip version updating and package installations and start working with your data right away? That would be amazing! And this is possible with the development of webr by George Stagg and colleagues https://github.com/r-wasm/webr.\n\nIt is R in the browser!\n\nThis is so great, because it provides the most convenient, quick and easy way to enter the R world. It is just like you having your Excel, Word and Powerpoint always immediately up and running by a click of a button. Since with R we type in our commands instead of pointing and clicking we are in the era of type-and-click to get your data science done.\nThis book is completely written using quarto and webr and allows you to typ in the code and run it right in the browser."
  },
  {
    "objectID": "jump_essentials.html#the-essentials",
    "href": "jump_essentials.html#the-essentials",
    "title": "2  Jumping essentials",
    "section": "2.1 The essentials",
    "text": "2.1 The essentials\nBefore you jumop into the water it can be of benefit when you know more about the water. What is the temperature? Is it really cold or just nice and warm? How high is the jump? Do you need to jump first 5 meters from a diving board or can you allready feel the water with your toes. This first chapter will give some basic programming essentials that will allow you to jump easier. Also it can be used as a reference for when you need to make the jump again\n\n2.1.1 Find your info online and in documentation\nR has so many functions that it is impossible to know everything by heart. So documentation of functions and the internet are always your best friend.\nStackexhange is an excellent resource. Almost 90 to 99% of your questions related to how you should use your R and tidy functions has been asked before by others. THe nice thing is that the active coding community put those questions with reproducible code in Stackexchange. More importantly, almost all questions has been accurately answered in multiple ways.\nOther resources that come up more often in my search results are either forums on POSIT community, Reddit, or Github discussions or issues can also be usefull, but these are more forum-like comments, with not such a good solvability structure as stackexchange.\nThen there are many more resources that somehow scrape the internet and collect basic info. Most of the time the info is correct but too simplistic. Not real issues are tackled. These are sites like geeksforgeeks, datanovia, towardsdatascience, some have better info then others, but most of the time these have commercial activities and in the end want to sell you courses or get your clicks.\n\n\n2.1.2 R and tidyverse documentation\nAll functions in R and tidyverse are accurately documented. All its arguments are described and especially the examples that are given are really helpful. Packages have often even more documentation called vignettes that explain certatin topics and contexts on how and when to use the functions.\n\n\n2.1.3 Style and layout\nWriting your code benefits from proper readability. Just like we layout our texts, manuscripts and excel data files, we also need a good layout for our code.\nLoading\n  webR...\n\n\n  \n\n\nThere are mulitple ways to organize your code, I try to adhere to: - short lines (max 60 characters per line) - indent after first line - indent after ggplot - each next function call aligns with the above function - each argument aligns with the previous argument - each ggplot layer gets its own line - I put the x and y aesthetics for ggplot mapping on one line\nOther good practices are: - use the package name before a function, like dplyr::mutate - use comments to annotate the code, when you put a # before it, it is not executed\nSo here is an example on what not to do and its corrections\nLoading\n  webR..."
  },
  {
    "objectID": "jump_essentials.html#basic-r-semantics",
    "href": "jump_essentials.html#basic-r-semantics",
    "title": "2  Jumping essentials",
    "section": "2.2 Basic R semantics",
    "text": "2.2 Basic R semantics\nWhen starting using R and tidyverse the new language can be daunting. So here is a short primer of common semantics that are often not directly understood from code.\nI took some of these example directly or indirectly from:\nhttps://uc-r.github.io/basics\n\n2.2.1 Assignment\nThe most common way of assigning in R is the <- symbol. Although the = works in the same way, it is reserved by R users for other things. I tend to use it for assigning numbers to constants, and it is used in function arguments\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.2 Vectors and lists\nA vector in R is a collectino of items (elements) of the same kind (types). A list is a collection of items to can also have different types. We make a vector with c() and a list with list. The c in c() apparently stands for combine link\nLoading\n  webR...\n\n\n  \n\n\nLoading\n  webR...\n\n\n  \n\n\nAlso, R forces that a vector is of one type. You can see that when you specifiy a vector with numbers and characters eg. c(1, 2, \"1\", \"2). It forces the vector to be of character type.\nLoading\n  webR...\n\n\n  \n\n\nLists form the basis of all other data than vectors. Dataframes are collections of related data with rows and columns and unique columns names and row names (or row numbers). data.frame is actually a wrapper around the list method.Tibbles are the tidyverse equivalent of dataframes with some more handy properties over dataframes. A ‘list’ can have names items or not.\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.3 Common semantics\nR language is different from other programming languages, and when starting out learning R there are some rules and common practices.\n\n\n2.2.4 ~ (the “tilde”)\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.5 + (the plus)\nApart from the simple arithmetic addition + is also used in the ggplot functions. It adds the multiple layers to each ggplot\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.6 %>% (the pipe)\nThe %>% is used to forward an object to another function or expression. It was first introduced in the magrittr package and is now also introduced in base R as the |> pipe, which are now identical. See blogpost for more info.\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.7 == (equal to)\nThe == is the equal to operator. It is different than = which is used only for assignment.\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.8 aes (aesthetics in ggplot)\nThe aes is important for telling the ggplot what to plot. aes are the aesthetics of the plot that need to mapped to data. So the ggplot needs data and mappings.\nThe ggplot acronym is actually coming from the grammar of graphics, which is a book “The grammar of graphics” by Leland Wilkinson, and was used by Hadley Wickham to make the ggplot package in 2005.\nA ggplot consists of: - data - aestehtic mappings (like x, y, shape, color etc) - geometric objects (like points, lines etc) - statistical transformations (stat_smooth) - scales - coordinate systems - themes and layouts - faceting\nLoading\n  webR...\n\n\n  \n\n\n\n\n2.2.9 %in% (match operator)\nThis is handy to check and filter specific elements from a vector\nLoading\n  webR..."
  },
  {
    "objectID": "jump_essentials.html#practical-tips",
    "href": "jump_essentials.html#practical-tips",
    "title": "2  Jumping essentials",
    "section": "2.3 Practical tips",
    "text": "2.3 Practical tips\n\n2.3.1 Running your code\nWebr code in the browser can be run as a complete code block by clicking on the Run code button when the webr status is Ready!, right above the block.\n\n\n\nScreenshot of a code block that is ready to run\n\n\nAnother option is to select a line of code (or more lines) and press command or ctrl enter. This will execute only the line or lines that you have selected.\n\n\n2.3.2 Simple troubleshooting your pipelines and ggplots\nIt happens that your code is not right away typed in perfectly, so you will get errors and warnings. It is good practice to break down your full code block or pipe into parts and observe after which line of code the code is not working properly.\n\n\n2.3.3 Building your data visualisation step by step\nLet’s take a built-in R dataset USArrests. We want to visualize how the relative number of murders in the state Massachusetts relates to the other states with the highest urban population in those state. In the dataset, the murder column represents the number of murders per 100.000 residents\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nExercise x\n\n\n\nMake a plot that addresses the above dataviz problem.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nHINTS\n\n\n\n\n\nHints:\nDo the following in your coding:\n\nglimpse at the data and look at the top5 rows using head()\nuse tibble::rownames_to_column() to make a separate column called states\nclean the column names using janitor::clean_names()\nturn the datatable into a tibble using ‘as_tibble’\ntake only the the top states by using a filter on the urban population (take it higher than 74)\nplot the data using a geom_col\nlabel the x axis and not the y-axis\nhighlight the massachusetts column using a separate geom_col layer, were you put a filter on the original data by using in the geom_col a call to `data = . %>% filter(str_detect(states, “Mass)). Also give this bar a red color.\napply a nice theme so that there are only x axis grid lines and no lines for y and x axis.\nAlso make sure that x-axis starts at zero\n\nInclude all these aspects step by step.\n\n\n\n\n\n\n\n\n\n\n\nSolution to Exercise x\n\n\n\n\n\nLoading\n  webR..."
  },
  {
    "objectID": "jump_cars.html",
    "href": "jump_cars.html",
    "title": "3  Plotting cars",
    "section": "",
    "text": "4 Practical tips"
  },
  {
    "objectID": "jump_cars.html#learn-and-code",
    "href": "jump_cars.html#learn-and-code",
    "title": "3  Plotting cars",
    "section": "3.1 Learn and code",
    "text": "3.1 Learn and code\nFirst, let’s make a simple scatter plot. We use a famous dataset that is used in R a lot for educational puposes. This is the mtcars dataset (mtcars = “Motor Trend Car Road Tests”). See parameter overview and documentation for info about the mtcars dataset. We can always call this dataset when using R, it is one of many datasets available from base R or tidyverse packages.\nFirst, we will inspect the dataset. For this we will load the tidyverse:\nLoading\n  webR...\n\n\n  \n\n\nOnce tidyverse is loaded via the library call, it is loaded in your current session in your browser, so you do not have to load it each time. Next have a look at the full dataset.\nLoading\n  webR...\n\n\n  \n\n\nor\nLoading\n  webR...\n\n\n  \n\n\nor\nLoading\n  webR...\n\n\n  \n\n\nor\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nThe pipe %>% operator\n\n\n\n\n\nThe %>% operator is a huge component of the tidy way of working. In R we now also have a “native” pipe that we can also use. This is the |> operator. For a history of the pipe in R see this blogpost.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\nLet’s select a small part of the data using select from the dplyr package:\nLoading\n  webR...\n\n\n  \n\n\nNext, make a simple plot with the miles per gallon (mpg) and dispension parameters (disp) in the mtcars dataset.\nLoading\n  webR...\n\n\n  \n\n\nThis is a very basic plot, without much formatting. Let’s make it prettier!\nAdd color and bring in a third parameter:\nLoading\n  webR...\n\n\n  \n\n\nHere we need to have a look at data-types. The cyl parameters is a numerical parameter. GGplot automatically assumes we want a continous scale for this. Instead the cyl is more of a categorical data type there are either 4, 6 or 8 cylinders in each car so we can make the cyl parameter categorical like this:\nLoading\n  webR...\n\n\n  \n\n\nIf you want to have different color you can use one of the many color pallettes that are available:\nLoading\n  webR...\n\n\n  \n\n\nApart from color you can change the shape of the datapoints:\nLoading\n  webR...\n\n\n  \n\n\nGGplot can use different themes for your plots and there are many many options to tweak your plots to the way you like\nLet’s change titles:\nLoading\n  webR...\n\n\n  \n\n\nChange the plotting theme and base size of the elements:\nLoading\n  webR...\n\n\n  \n\n\nChange the scaling of the axes. It is good practice to plot graphs from zero:\nLoading\n  webR...\n\n\n  \n\n\nNow the datapoints at the maxima of the axis are not completely visible so it would be nice that we have some more space:\nLoading\n  webR...\n\n\n  \n\n\nNow we have generated a nice visualisation of our data using ggplot. Please note that ggplot uses layers and we added each time a different layer of information to the ggplot. If you want you can go wild with ggplot. Please find a nice overview of visualisations using ggplot, tidy and R from Cedric Scherer. Also the underlying R code is available for those plots."
  },
  {
    "objectID": "jump_cars.html#exercises",
    "href": "jump_cars.html#exercises",
    "title": "3  Plotting cars",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\n\n3.2.1 Adding layers and changing the MTCARS plot\n\n\n\n\n\n\nExercise 1\n\n\n\nGive the points in the ggplot some transparency (or opacity), so that individual points are better visible. TIP: use the alpha argument it should be a number from 0 to 1.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 1\n\n\n\n\n\nPlease note that the alpha we added is not part of an aesthetics (aes), meaning that the value of the alpha is not linked with a parameter in our data.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nAdd a layer that will generate a smooth linear regression line that shows the relation between mpg and disp. Use the stat_smooth command for this.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 2\n\n\n\n\n\nPlease make sure that the ggplot is not separates into groups, if the data is grouped by color or shape the regression line for each group will be generated.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nUse the facet_wrap command to make three separate plots for each cylinder.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 3\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n3.2.2 Fixing common errors\nBelow is some code that is not working properly, because of coding semantics mistakes. Can you spot (and fix) the errors?\n\n\n\n\n\n\nFix error 1\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Error 1\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nCommas are often forgotten, but easily fixed. Within brackets arguments are separated with commas. R also generates an error that is helpful and can point you to the missing ,.\n\n\n\n\n\n\n\n\n\nFix error 2\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Error 2\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nGgplot layers are added with a + not with the pipe term.\n\n\n\n\n\n\n\n\n\nFix error 3\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Error 3\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nMake sure that the end of a layer or line of code is not followed up with a + or %>%.\n\n\n\n\n\n\n\n\n\nFix error 4\n\n\n\nThis is an error that is not apparent from an error message that R generates for you. However, the code does not give you what you want. The plot should show the cyl parameter in different shapes, just like there are three different colors for each level of the cyl parameter.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Error 4\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nThe shape argument should be included in the aesthetics (aes) part of the ggplot\n\n\n\n\n\n\n\n\n\nSolution to Error 4 using chatGPT\n\n\n\n\n\nSince chatGPT can also solve basic programming problems https://doi.org/10.1371/journal.pcbi.1011511, I tried solving Error 4 also with chatGPT. In my chatGPT session, the following solution was given:\nLoading\n  webR...\n\n\n  \n\n\nchatGPT indeed put the shape in the aes, but also started making other changed. The remark that chatGPT made was: “you should correct the syntax error and use scale_shape_manual() instead of scale_shape().”… “to customize the shape values and labels.”. I didn’t really ask for that, and from there it went all downhill. Let’s call that the “chatGPT loop of death”. Let’s see what happened.\nRunning the “improved” code showed that there was still no fill, so it did not improve in a way that I wanted.\nNext I said “but now the shapes don’t have a fill” and chatGPT replied:\n“I see your concern. If you want to have both color and fill for the shapes, you can achieve this by mapping fill within the aes() call and using scale_fill_brewer() for the fill scale. Here’s the modified code to achieve that:”\nLoading\n  webR...\n\n\n  \n\n\nThis again did not give fill unfortunately. Asking for a different solution: “the fill is not there, do you have another solution?”\nLoading\n  webR...\n\n\n  \n\n\nAlso did not give me fill.\nAnother attempt by chatGPT (still not working):\nLoading\n  webR...\n\n\n  \n\n\nAnother attempt by chatGPT (still not working):\nLoading\n  webR...\n\n\n  \n\n\nAsking chatGPT to use scale_shape and solid=TRUE gives:\nLoading\n  webR...\n\n\n  \n\n\n…. but apparently chatGPT doesn’t know how to use scale_shape (in the first instance), because it scale_shape cannot take values as an argument. So when I reply with the following “I get the following error: Error: unused argument (values = c(15, 16, 17))”, chatGPT goes back to use the ’scale_fill_manual`.\nLoading\n  webR...\n\n\n  \n\n\nAND FINALLY we have a nice and simple fill with one legend…… but also lots of unnecessary code. So after a few exchanges to make the code more concise this is what came out of it:\nLoading\n  webR...\n\n\n  \n\n\nAffter asking to use scale_shape instead of scale_shape_manual, chatGPT generates:\nLoading\n  webR...\n\n\n  \n\n\nThis works nicely, but chatGPT introduces stroke = 1, which is not needed here, so again we have unnecessary code. So after I asked “can I leave out the stroke argument?” we get the easiest solution and exactly the same solution as I came up with myself without chatGPT.\nLoading\n  webR...\n\n\n  \n\n\nPlease note, that when building the ggplot example, I did use google (…off course) to get some solutions, I liked the scale_shape and solid=TRUE solution that I found, because it made the code so concise and I don’t like to type in values and breaks manually.\nChatGPT use in science and coing is just dipping the toe in the water. ChatGPT is likely to better not be used as knowledge database but instead as “reasoning or infering agents” https://www.nature.com/articles/s41591-023-02594-z. ChatGPT can produce false information, also described as “hallucinations” https://www.nature.com/articles/d41586-023-00816-5, which makes it difficult to use it for getting knowledge and facts. That said, it can be used to gain knowledge and learn better coding skills. Here is a nice quick tips paper from PLOS computational biology on how to “harness the power of chatGPT” https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011319."
  },
  {
    "objectID": "jump_cars.html#basic-r-semantics",
    "href": "jump_cars.html#basic-r-semantics",
    "title": "3  Plotting cars",
    "section": "3.3 Basic R semantics",
    "text": "3.3 Basic R semantics\nWhen starting using R and tidyverse the new language can be daunting. So here is a short primer of common semantics that are often not directly understood from code.\nI took some of these example directly or indirectly from:\nhttps://uc-r.github.io/basics\n\n3.3.1 Assignment\nThe most common way of assigning in R is the <- symbol. Although the = works in the same way, it is reserved by R users for other things. I tend to use it for assigning numbers to constants, and it is used in function arguments\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.2 Vectors and lists\nA vector in R is a collectino of items (elements) of the same kind (types). A list is a collection of items to can also have different types. We make a vector with c() and a list with list. The c in c() apparently stands for combine link\nLoading\n  webR...\n\n\n  \n\n\nLoading\n  webR...\n\n\n  \n\n\nAlso, R forces that a vector is of one type. You can see that when you specifiy a vector with numbers and characters eg. c(1, 2, \"1\", \"2). It forces the vector to be of character type.\nLoading\n  webR...\n\n\n  \n\n\nLists form the basis of all other data than vectors. Dataframes are collections of related data with rows and columns and unique columns names and row names (or row numbers). data.frame is actually a wrapper around the list method.Tibbles are the tidyverse equivalent of dataframes with some more handy properties over dataframes. A ‘list’ can have names items or not.\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.3 Common semantics\nR language is different from other programming languages, and when starting out learning R there are some rules and common practices.\n\n\n3.3.4 ~ (the “tilde”)\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.5 + (the plus)\nApart from the simple arithmetic addition + is also used in the ggplot functions. It adds the multiple layers to each ggplot\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.6 %>% (the pipe)\nThe %>% is used to forward an object to another function or expression. It was first introduced in the magrittr package and is now also introduced in base R as the |> pipe, which are now identical. See blogpost for more info.\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.7 == (equal to)\nThe == is the equal to operator. It is different than = which is used only for assignment.\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.8 aes (aesthetics in ggplot)\nThe aes is important for telling the ggplot what to plot. aes are the aesthetics of the plot that need to mapped to data. So the ggplot needs data and mappings.\nThe ggplot acronym is actually coming from the grammar of graphics, which is a book “The grammar of graphics” by Leland Wilkinson, and was used by Hadley Wickham to make the ggplot package in 2005.\nA ggplot consists of: - data - aestehtic mappings (like x, y, shape, color etc) - geometric objects (like points, lines etc) - statistical transformations (stat_smooth) - scales - coordinate systems - themes and layouts - faceting\nLoading\n  webR...\n\n\n  \n\n\n\n\n3.3.9 %in% (match operator)\nThis is handy to check and filter specific elements from a vector\nLoading\n  webR..."
  },
  {
    "objectID": "jump_cars.html#running-your-code",
    "href": "jump_cars.html#running-your-code",
    "title": "3  Plotting cars",
    "section": "4.1 Running your code",
    "text": "4.1 Running your code\nWebr code in the browser can be run as a complete code block by clicking on the Run code button when the webr status is Ready!, right above the block.\n\n\n\nScreenshot of a code block that is ready to run\n\n\nAnother option is to select a line of code (or more lines) and press command or ctrl enter. This will execute only the line or lines that you have selected."
  },
  {
    "objectID": "jump_cars.html#simple-troubleshooting-your-pipelines-and-ggplots",
    "href": "jump_cars.html#simple-troubleshooting-your-pipelines-and-ggplots",
    "title": "3  Plotting cars",
    "section": "4.2 Simple troubleshooting your pipelines and ggplots",
    "text": "4.2 Simple troubleshooting your pipelines and ggplots\nIt happens that your code is not right away typed in perfectly, so you will get errors and warnings. It is good practice to break down your full code block or pipe into parts and observe after which line of code the code is not working properly."
  },
  {
    "objectID": "jump_cars.html#building-your-data-visualisation-step-by-step",
    "href": "jump_cars.html#building-your-data-visualisation-step-by-step",
    "title": "3  Plotting cars",
    "section": "4.3 Building your data visualisation step by step",
    "text": "4.3 Building your data visualisation step by step\nLet’s take a built-in R dataset USArrests. We want to visualize how the relative number of murders in the state Massachusetts relates to the other states with the highest urban population in those state. In the dataset, the murder column represents the number of murders per 100.000 residents\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nExercise x\n\n\n\nMake a plot that addresses the above dataviz problem.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nHINTS\n\n\n\n\n\nHints:\nDo the following in your coding:\n\nglimpse at the data and look at the top5 rows using head()\nuse tibble::rownames_to_column() to make a separate column called states\nclean the column names using janitor::clean_names()\nturn the datatable into a tibble using ‘as_tibble’\ntake only the the top states by using a filter on the urban population (take it higher than 74)\nplot the data using a geom_col\nlabel the x axis and not the y-axis\nhighlight the massachusetts column using a separate geom_col layer, were you put a filter on the original data by using in the geom_col a call to `data = . %>% filter(str_detect(states, “Mass)). Also give this bar a red color.\napply a nice theme so that there are only x axis grid lines and no lines for y and x axis.\nAlso make sure that x-axis starts at zero\n\nInclude all these aspects step by step.\n\n\n\n\n\n\n\n\n\n\n\nSolution to Exercise x\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n4.3.1 Find your info online and in documentation\nR has so many functions that it is impossible to know everything by heart. So documentation of functions and the internet are always your best friend.\nStackexhange is an excellent resource. Almost 90 to 99% of your questions related to how you should use your R and tidy functions has been asked before by others. THe nice thing is that the active coding community put those questions with reproducible code in Stackexchange. More importantly, almost all questions has been accurately answered in multiple ways.\nOther resources that come up more often in my search results are either forums on POSIT community, Reddit, or Github discussions or issues can also be usefull, but these are more forum-like comments, with not such a good solvability structure as stackexchange.\nThen there are many more resources that somehow scrape the internet and collect basic info. Most of the time the info is correct but too simplistic. Not real issues are tackled. These are sites like geeksforgeeks, datanovia, towardsdatascience, some have better info then others, but most of the time these have commercial activities and in the end want to sell you courses or get your clicks.\n\n\n4.3.2 R and tidyverse documentation\nAll functions in R and tidyverse are accurately documented. All its arguments are described and especially the examples that are given are really helpful. Packages have often even more documentation called vignettes that explain certatin topics and contexts on how and when to use the functions.\n\n\n4.3.3 Style and layout\nWriting your code benefits from proper readability. Just like we layout our texts, manuscripts and excel data files, we also need a good layout for our code.\nLoading\n  webR...\n\n\n  \n\n\nThere are mulitple ways to organize your code, I try to adhere to: - short lines (max 60 characters per line) - indent after first line - indent after ggplot - each next function call aligns with the above function - each argument aligns with the previous argument - each ggplot layer gets its own line - I put the x and y aesthetics for ggplot mapping on one line\nOther good practices are: - use the package name before a function, like dplyr::mutate - use comments to annotate the code, when you put a # before it, it is not executed\nSo here is an example on what not to do and its corrections\nLoading\n  webR..."
  },
  {
    "objectID": "jump_seahorse.html",
    "href": "jump_seahorse.html",
    "title": "4  Plotting seahorse",
    "section": "",
    "text": "Now, lets plot some Seahorse data. For this we need to import some here into this session. We use a dataset that was we published in Scientific Reports Janssen et al.. It is data from PBMCs where we followed OCR and ECAR using Extracellular Flux analysis with the XFe96 over time and during that time we injected after three measurement phases FCCP, and after six measurement phases we injected Antimycin/Rotenone (AM/Rot). The data is available from github.\nLoading\n  webR...\n\n\n  \n\n\nAs you can see from the glimpse, the data table that we have now (we call it a tibble in tidy lanuguage), contains 7 columns; Measurement, Well, Group, Time, OCR, ECAR, PER. The data is allready nice and tidyly organized in the Rate sheet of the excel file that we have loaded. The file was generated in the Wave Agilent software and directly comes from exporting the Seahorse data to xlsx.\nI prefer to use lower case column names without any spaces, so for these column names we have to turn them into lower case first. We use some easy functions from the janitor package for this.\nLoading\n  webR...\n\n\n  \n\n\nNext, we can start plotting data using ggplot. Let’s introduce the filter command from dplyr. Whereas select is there to select columns, filter is there to select rows. So let’s filter the rows for the group with is labeled “200.000” (200.000 cells/per well) and the “Background” group.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nThe filter command\n\n\n\n\n\nFiltering data is selecting the rows based on some arguments. You need some to understand some semnatics here. For filtering based on multiple conditions we use group %in% c(\"200.000\", \"Background\"), for filtering based on a single condition we can use group == \"200.000\". The %in% operator is used to match two items.\n\n1 %in% c(1,2,3,4,5) #is TRUE\n\n[1] TRUE\n\n\n\n# just like\n1 == 1 #is TRUE\n\n[1] TRUE\n\n\n\n#the reverse is also possible\nc(1,2,3,4,5) %in% 1\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\n#is TRUE FALSE FALSE FALSE FALSE FALSE\n\nLoading\n  webR...\n\n\n  \n\n\nLoading\n  webR...\n\n\n  \n\n\nThus the group %in% c(\"200.000\", \"Background\") statement in the filter function above tells which group items to use. For 200.000 there is match (TRUE), but for 100.000 there is not a match (it is FALSE).\n\n\n\nNow that we know how to filter we can use the filtered data to make the ggplot.\nLoading\n  webR...\n\n\n  \n\n\nThat plot is not so informative. Let’s make it prettier. First, add a line plot:\nLoading\n  webR...\n\n\n  \n\n\nNext, change colors:\nLoading\n  webR...\n\n\n  \n\n\nChange theme and text size:\nLoading\n  webR...\n\n\n  \n\n\nAdd titles:\nLoading\n  webR...\n\n\n  \n\n\nThis is a very nice plot. It shows all OCR curve for each well for the 200.000 and the background groups. The information that is now not in the plot is which line matches to which well.\nExercise We can color each line (but there are too many wells so it will not be nice!). * Change this in the above code color = well instead of color = group. * You will notice that there are not enough colors in the brewer palette Set1, so you go back to the default coloring by deleting the scale_color_brewer line as well. Use th # to comment out the line. * Now notice that the legend is huge and not completely visible, againg indicating that this is not the way to go\nInstead, we can try to label the lines. The ggrepel package is not yet available in webr, so for now we have to use the geom_text or annotate commands from ggplot. The benefit of ggrepel is that it automatically prevents text overlap.\nLoading\n  webR...\n\n\n  \n\n\nAlthough we now labeled lines that are at the minimum and maximum OCR, this is only usefull for this one plot in these conditions. The position of the label is tweaked based on this specific plot, making this not such a quick solution to our problem.\n\n\n\n\n\n\nSubsetting of data within the ggplot commands\n\n\n\n\n\nIn the above ggplot commands, we included the geom_text, but we only used a subset of the full data for this geom. We use the . (dot) operator to get the original data (so in our case the filtered data that went into the ggplot), and piped that into another two filters. Basically we do the following, but then within one layer of the ggplot:\nLoading\n  webR...\n\n\n  \n\n\nThus here we are filtering all the way to getting only one row of the full dataset. The well name “C08” or “B08” is then given to the label argument of geom_text.\n\n\n\nLet’s do some more layout adjustments. Although the theme_bw gives a basic plotting layout, we often want to change the formatting. There are again great resources for this, for example this one: https://ggplot2.tidyverse.org/articles/faq-customising.html, but we explain the basics here. By giving options to the theme function we can change specific elements of a ggplot.\nFor example, if we want to change the text size of the axis title (or leave it blank), we give arguments to the axis.title options. Also please note the rel(1.2) argument which means relative 1.2 times higher than base_size. I think it is good practive to use the rel here instead of absolute numbers.\nLoading\n  webR...\n\n\n  \n\n\nChange the rel 1.2 to 0.5 in the above code and see what happens.\nNext, we change the grid lines\nLoading\n  webR...\n\n\n  \n\n\nNext, we change the orientation of the x axis labels.\nLoading\n  webR...\n\n\n  \n\n\nWe can also try to use the ggiraph package. This brings in some nice interactivity into the plot. Since we are now working with the plot in a browser, this can be very handy. Also if we want to publish the plot as html and not a plain PDF this can be usefull. ggiraph is unfortunately also not available for wasm/webr since one dependent package is not available uuid, and I also can’t get it to run via quarto…..\nSo let’s try something else in a couple of exercises.\n\n\n\n\n\n\nExercise 1\n\n\n\nAdd three vertical lines to the plot. You can use the geom_vline command with xintercepts set at 15, 33 and 48; so that the the line is approximately at the injection time point. Also give it a shade of grey, eg. grey40.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 1\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nNow add the injection labels. Use the annotate command and\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 2\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nUse the facet_wrap command to plot all groups (except background) in separate plots and in each plot show the wells. First, we will need to filter away the background data. Instead of selecting all groups we need it is better and easier to this filter our the background data using filter(group != \"Background\"). The != means “is not” this is the reverse of the == operator.\nNext, add the facet_wrap command to the ggplot. I prefer to do that always at the bottom, so that I can easily see if a plot is wrapped.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 3\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nThe plot in exercise 3 looks great allready, but the order of the plots is important! We would like to see it go from low to high OCR. We can fix that using the forcats package commands. A nice and quick way to sort is based on the name of the group. Important to realize is that the Group column in the XF data are characters and not numbers. That is also the reason why that does not sort in the most natural way. It is sorted based on the first character, thus the “50.000” group comes last. If we would change the “group” column to double (that is a number format), it would sort better, but also your group name will change because it will recognize the . as a decimal operator. So it is better to leave the group names as they are and do it differently.\nIn comes forcats, you can relevel and reorder the crap out of your data in the ggplot! We often do the releveling at the point where you use your parameter, without making any changes the type of the columns. So that means you can use ~fct_reorder(group, group) in the facet_wrap instead of only ~group.\nPlease note that fct_reorder first argument is the parameter that you plot or need, and the second argument is the parameter that is used for sorting the data. In our case now that is the same, both are “group”, but we also need to add something else. If we would do it like this there will be no difference from when ggplot takes facet_wrap only takes ~group. Thus we can make the second argument into a number by using as.double.\nLoading\n  webR...\n\n\n  \n\n\nAlso, try in the above code what happens if you: - only use as.double in the facet_wrap - change the type of data to double for the group column\n\n\n\n\n\n\n\n\nSolution to Exercise 4\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nNow that the facet_wrap is sorted nicely, we would also like to have the legend sorted nicely. Use the same fct_reorder trick to reorder the color legend.\nLoading\n  webR...\n\n\n  \n\n\nIf you didn’t allready change the title of the legend, do that as well. You can specify the name of the legend manually using the name argument in the scale_color_brewer command.\n\n\n\n\n\n\n\n\nSolution to Exercise 5\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nPlease change the facet_wrap command so that the y-axis is not fixed for all groups. Make the output so that each individual plot has its own y-axis scale.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 6\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nNow, it is up to you to build a whole ggplot using the XF data. Instead of plotting time vs OCR, now plot cell density vs maximal capacity. For this you need to know some stuff.\n\nwe define maximal capacity as the OCR at measurement 4\nwe should filter out the “Background” group\nwe should convert the group names to numbers\nwe can also add the mean of all wells for each group by using: stat_summary()\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 7\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nThe previous plot showed the data from individual wells as well as the median for that group. You can also calculate the median before plotting using the dplyr summarize command. You can find summarize info here: https://dplyr.tidyverse.org/reference/summarise.html\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 8\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nWe can also perform a linear regression on the maximal capacity at different densities. For this we can use the geom_smooth command. The arguments should be method = \"lm\" and formula = y~x.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 9\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nObserve also what the difference is when only using the data from row A and H. You can uncomment the line in the above code. Please note I use the very usefull str_detect function for this from the stringr package that is also in the tidyverse.\n\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nNext, you can decide yourself what you want to plot. Have a glimpse at the data and think of another important visualisation that you want to make using all the tools that you have learned so far, or the tools that you found on the internet.\nLoading\n  webR..."
  },
  {
    "objectID": "jump_summary.html#what-did-we-learn",
    "href": "jump_summary.html#what-did-we-learn",
    "title": "5  Summary",
    "section": "5.1 What did we learn?",
    "text": "5.1 What did we learn?\n\n5.1.1 Ditching\n\nThe benefits of using R over point-and-click software for data analysis in biological and biomedical sciences are that it is open-source, it has a wide and diverse community with a huge number of resources, it is relatively easy to learn, and it offers workflows that is very well suited for doing reproducible and responsible data analysis.\nThe tidyverse offers advantages over base R. It offers an intuitive way of coding with functional names and tidy data handling and coding in mind\nR in the browser offers easy access to R without installing software\n\n\n\n5.1.2 Cars\n\nGeneral R coding and execution of code\nHow to look at data tables: head, tail, glimpse\nThe pipe operator %>% or |>\nMaking factorial data using as.factor\nthe dplyr function select\nbasic ggplot functions using aes aesthetics and geoms such as geom_point\nadding color and shape and using scale_brewer_manual and scale_shape\nimproving layout; theme_bw, base_size and labs\nusing chatGPT for coding improvements\n\n\n\n5.1.3 Seahorse\n\nLoading data and working with typical Seahorse data\nUsing janitor clean_names\nUsing the dplyr function filter\nUsing the %in% operator\nChanging the layout of ggplots usine theme elements and arguments.\nadding text to ggplot using geom_text and annotate\nAdding lines to ggplot using geom_vline\nnesting pipes in ggplot function for subsetting data\nUsing facet_wrap to make multiple similar plots from one datatable\nUsing the forcats fct_reorder function\nChanging data formats to numbers using as.double\nUsing the dplyr summarize function\nUsing stat_summary to compute means or medians in ggplots\nUsing geom_smooth to make regression lines"
  },
  {
    "objectID": "jump_summary.html#what-we-did-not-learn",
    "href": "jump_summary.html#what-we-did-not-learn",
    "title": "5  Summary",
    "section": "5.2 What we did not learn?",
    "text": "5.2 What we did not learn?\n\nbase R functions and how to address data in base R, eg xf$OCR[xf$Group == \"Background] and xf$Well[10]\nother important tidyverse functions, like pivot_wider, pivot_longer,\nmore complicated functions like the map function from the purrr package\nother simple ggplot geoms, like geom_bar, geom_boxplot, geom_density\nhow to save images and plots for using them in other software"
  },
  {
    "objectID": "swim.html",
    "href": "swim.html",
    "title": "Swim underwater",
    "section": "",
    "text": "Now we will go for a bit more than just swimming in R code. We will go underwater and swim in shallow water to look at those seahorses. In this section we will introduce functions from the seahtrue package and explore the output of seahtrue functions."
  },
  {
    "objectID": "swim_functions.html#functions",
    "href": "swim_functions.html#functions",
    "title": "6  Seahtrue functions",
    "section": "6.1 Functions",
    "text": "6.1 Functions\nFirst, let’s see what a function is in R. In the previous section, we used functions that changed our data, eg. filter, select and clean_names. Functions are just a bunch of code lines using any code and other functions you want to accomplish a task. Here are some formal definitions:\n\nFunctions are “self contained” modules of code that accomplish a specific task. Functions usually take in some sort of data structure (value, vector, dataframe etc.), process it, and return a result. link\n\n\nA function in R is an object containing multiple interrelated statements that are run together in a predefined order every time the function is called. link\n\nFunctions take arguments, these are used as input for your function.\nLoading\n  webR...\n\n\n  \n\n\nPlease note that it is good practice to use verbs in function names and address in the name what a function is doing. In our case we define a function change_mtcars_cyl_to_x because this is exactly what this function is doing.\n\n\n\n\n\n\nExercise 1\n\n\n\nThe change_mtcars_cyl_to_x function in the above code is a nonsensical function, because you never want to change a column to one specific value. Let alone a specific column named cyl. Also, you don’t have to write a whole separate function for this, you can also directly use the mutate function from dplyr. Write the code without using the change_mtcars_cyl_to_x function, but achieve the same result.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 1\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nPlease note that you can change the data in a column to anything you want. R is very very flexible in datatypes (compared to other languages). So if you would do this:\nLoading\n  webR...\n\n\n  \n\n\nthat is also fine.\nThe data types are given when you glimpse the data.\nLoading\n  webR...\n\n\n  \n\n\nYou see that all columns are <dbl> which stands for double, which is a numeric data type. integer is another common numerical datatype.\nWhen you replace the cyl column data with \"eight\", which is of the character type, the data type will change.\nLoading\n  webR...\n\n\n  \n\n\nThis is all fine in R. Important to note though is that a column can have only one data type. In Excel you can define each cell a different data type, but in R that is not possible. So it is either a column of type character or double in our case.\n\n\n\nNow let’s extend the function a bit to have two arguments:\nLoading\n  webR...\n\n\n  \n\n\nAlthough the function is a bit more general, because we can now also input the tibble that we want to change, it is still not very useful in practice. A single mutate function is preferred to be used here. On the other hand it is an easy example to demonstrate what a function is and how it works."
  },
  {
    "objectID": "swim_functions.html#seahtrue-read-data-function",
    "href": "swim_functions.html#seahtrue-read-data-function",
    "title": "6  Seahtrue functions",
    "section": "6.2 Seahtrue read data function",
    "text": "6.2 Seahtrue read data function\nNow let’s start with the functions from the seahtrue package. Since seahtrue is not available for webr, we need to load in the functions manually. The first thing we do is to read data from the excel file that is generated using the Wave software. In the previous section we only loaded in one sheet of that datafile Rate, but the seahtrue package takes all data and organizes it nicely (and tidyly) into a nested tibble.\nOne of the functions is the get_xf_raw. It reads the Raw sheet from the excel file.\n\nget_xf_raw <- function(fileName){\n  \n  xf_raw <- readxl::read_excel(fileName, sheet = \"Raw\")\n  \n}\n\nThe argument fileName and its location is important. If we work with data input for your scripts, you need to be precise where your files are located. On windows and mac computers and with cloud services and web apps, it can get confusion what this exact location is of your files, either locally or on network or cloud. Sometimes they are on the desktop or in a documents folder, or they can live on a network drive. Properly addressing these files can be difficult because the full path is not always known. It is often recommended to put data files in the Rstudio project folder that you work with, so that you can work with relative paths from your project root directory. This is another example of good practice.\nUsing webr/wasm we do a similar thing, we download the file to our local drives. On my computer when I download a file it goes into the /home/web_user/ directory. Apparently this is my working directory in my webr/wasm sessions. Since it is the working directory, everything that is in there is directly accessible with only the filename. You don’t need a full path name like C:\\Users\\MyName\\Desktop\\R\\projects\\blabla\\datafolder\\data\\. So for the get_xf_raw function to work we first download the file into our session working directory and then we call the get_xf_raw function.\nLoading\n  webR...\n\n\n  \n\n\nPlease note that we also tell our session what the get_xf_raw function is here. Basically, we assign the code lines to get_xf_raw, so when we call get_xf_raw with its argument, these lines of code are run.\n\n\n\n\n\n\nusing functions from packages\n\n\n\n\n\nIn the get_xf_raw function we call the read_xlsx function. However, we also include the package from which the function is from, like this readxl::read_xlsx. This has two advantages. First, it is good practice to show where your function comes from, because sometimes a function name is used in mulitple different packages. For example, the filter function we often use is from dplyr, but the stats package also uses the filter function but then in a slightly different way. Second, when using the package::function annotation you don’t have to load the package using the library command.\nIn other languages, such as python, you are required to also include the library, when calling a function from that library https://www.rebeccabarter.com/blog/2023-09-11-from_r_to_python.\n\n\n\nApart from reading the Raw data sheet there are a couple more functions to read the other data and meta info.\n\n#raw data\nget_xf_raw()\n\n#rate data\nget_xf_rate()\n\n#normalization data\nget_xf_norm()\n\n#buffer factors\nget_xf_buffer()\n\n#injection info\nget_xf_inj()\n\n#pH calibration data\nget_xf_pHcal()\n\n#O2 calibration data\nget_xfO2cal()\n\n#flagged wells\nget_xf_flagged()\n\n#assay info\nget_xf_assayinfo()\n\nFurthermore, there is a function that combines all functions as above and outputs them in a list:\n\n#raw data\nread_xf_plate()\n\nThe input argument for all is the filename or path of the input xlsx data file."
  },
  {
    "objectID": "swim_functions.html#seahtrue-preprocess-data-function",
    "href": "swim_functions.html#seahtrue-preprocess-data-function",
    "title": "6  Seahtrue functions",
    "section": "6.3 Seahtrue preprocess data function",
    "text": "6.3 Seahtrue preprocess data function\nFollowing reading the data, the data needs to be processed to a tidy format so that it can be easily used for downstream processing.\nFor example, there is a function which changes the columns from the input file data into names without capitals and spaces. The clean_names from the janitor package can also be used, but in this case we wanted to be a bit more precise on what the names should be.\n\nrename_columns <- function(xf_raw_pr) {\n\n  # change column names into terms without spaces\n  colnames(xf_raw_pr) <- c(\n    \"measurement\", \"tick\", \"well\", \"group\",\n    \"time\", \"temp_well\", \"temp_env\", \"O2_isvalid\", \"O2_mmHg\",\n    \"O2_light\", \"O2_dark\", \"O2ref_light\", \"O2ref_dark\",\n    \"O2_em_corr\", \"pH_isvalid\", \"pH\", \"pH_light\", \"pH_dark\",\n    \"pHref_light\",\n    \"pHref_dark\", \"pH_em_corr\", \"interval\"\n  )\n\n  return(xf_raw_pr)\n}\n\nThe next preprocessing function takes the timestamp (colnumn name is now time) from the Raw data sheet and converts the timestamp into minutes and seconds. This function has some more plines of code, but all it does is to add three columns to the tibble: totalMinutes, minutes and timescale. I used timescale here to make sure that I can recognize it as different from the time column.\n\nconvert_timestamp <- function(xf_raw_pr) {\n\n  # first make sure that the data is sorted correctly\n  xf_raw_pr <- dplyr::arrange(xf_raw_pr, tick, well)\n\n  # add three columns to df (totalMinutes, minutes and time) by converting the timestamp into seconds\n  xf_raw_pr$time <- as.character((xf_raw_pr$time))\n  times <- strsplit(xf_raw_pr$time, \":\")\n  xf_raw_pr$totalMinutes <- sapply(times, function(x) {\n    x <- as.numeric(x)\n    x[1] * 60 + x[2] + x[3] / 60\n  })\n  xf_raw_pr$minutes <- xf_raw_pr$totalMinutes - xf_raw_pr$totalMinutes[1] # first row needs to be first timepoint!\n  xf_raw_pr$timescale <- round(xf_raw_pr$minutes * 60)\n\n  return(xf_raw_pr)\n}\n\nAll other preprocessing steps and functions can be looked up in the preprocess_xfplate.R file on github https://github.com/vcjdeboer/seahtrue/blob/develop-gerwin/R/preprocess_xfplate.R. Combined the preprocess_xfplate function takes the output of the read_xfplate function and outputs all data in a nice data table consisting of a bunch of nested tibbles.\nThe preprocess_xfplate and read_xfplate functions are combined in the run_seahtrue function. The seahtrue has some extensive unit testing, user interaction, and input testing build-in using the testthat, cli, logger and validate.\nThe basic read and prepocess function looks like this.\n\nrun_seahtrue() <- function(filepath_seahorse){\n  \n  filepath %>%\n    read_xfplate() %>%\n    preprocess_xfplate()\n}\n\nIn the next section we will explore the output of the run_seahtrue function.\n\n\n\n\n\n\nExercise 2\n\n\n\nThe rename_columns function could have also been written using clean_names from the janitor package. This would have been likely faster to implement. Replace a clean_names code that does the same as the rename_columns function\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 2\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nDo the same for the convert_timestamp function. Use the lubridate package to write a simpler code\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 3\n\n\n\n\n\nLoading\n  webR..."
  },
  {
    "objectID": "swim_outputs.html#nested-tibbles",
    "href": "swim_outputs.html#nested-tibbles",
    "title": "7  Seahtrue outputs",
    "section": "7.1 Nested tibbles",
    "text": "7.1 Nested tibbles\nThe data output format of the run_seahtrue function is a list of lists. List of lists is also called nesting of data. The advantage of this is that the data is properly organized, but also easily accessible. Here is an example that I took from a tidyr vignette https://tidyr.tidyverse.org/articles/nest.html.\nLoading\n  webR...\n\n\n  \n\n\nYou can see that the the data is now nicely organized by the cylinder parameter. Since there are only 3 different values for the cyl in the mtcars dataset, there are now three rows and two columns, one column has the cyl parameter all other data is nested into a data column.\n\n\n\n\n\n\n.by vs group_by\n\n\n\n\n\nIn one of the latest releases of the tidyverse the use of .by was introduced. Previously we used the group_by to tell R how to organize the data. The grouping of data remains attached to the data tibble, which sometimes could result in unintentional things to happen, when you forgot that the tibble was grouped. The group_by can be undone with the ungroup command.\nWith the .by the grouping is only apparent while using the function in which you use it as argument. group_by and .by are doing similar things so they can be used both.Let’s have a look at how they work:\nLoading\n  webR...\n\n\n  \n\n\nIf you glimpse the results of the two ways of using grouping above you will see that group_by is doing stuff to your data, that you might not want. In this case it turns the mtcars dataframe into a tibble, whereas the result of the .by in the summarize function is still a dataframe. Although it might not really matter whether your data is a tibble or dataframe, it shows that group_by is a bit more invasive on your data.\n\n\n\nYou can use pluck to get to the nested data. Basically you just pluck a part of the data out of the full dataset.\nLoading\n  webR...\n\n\n  \n\n\nPlease note that we use here \"data\" instead of data. It can be confusing when to use the \"\" or not. For example, with the pull function which takes one full column out of a tibble, you are not using \"\".\nAlso, pluck uses indexing for retrieving its components, it is not possible to directly get the element that belongs to cyl == 3 for example. You would need to filter first on that parameter and then pluck the first row of data."
  },
  {
    "objectID": "swim_outputs.html#the-purrr-map-function",
    "href": "swim_outputs.html#the-purrr-map-function",
    "title": "7  Seahtrue outputs",
    "section": "7.2 The purrr map function",
    "text": "7.2 The purrr map function\nThe cool thing about a nested tibble is that you can quickly perform stuff on each nested tibble. A really good introduction to this is described in this blog post by Rebecca Barter https://www.rebeccabarter.com/blog/2019-08-19_purrr. You can map a function on each item from that row.\nLoading\n  webR...\n\n\n  \n\n\nYou see that a new column is generated named model, if you pluck the one of the models, you can see the typical output of the linear model (lm) function. For each cylinder now you creates a linear model!\nLoading\n  webR...\n\n\n  \n\n\nThe semantics and how to use the map function is nicely explained in the blog post that was referenced here above. But some more considerations here:\nLoading\n  webR...\n\n\n  \n\n\nAnother good resource for the purrr map function is https://dcl-prog.stanford.edu/purrr-basics.html. map has many more forms and ways to use, which are summarized in its cheat sheet https://github.com/rstudio/cheatsheets/blob/main/purrr.pdf."
  },
  {
    "objectID": "swim_outputs.html#the-seahtrue-ouput",
    "href": "swim_outputs.html#the-seahtrue-ouput",
    "title": "7  Seahtrue outputs",
    "section": "7.3 The seahtrue ouput",
    "text": "7.3 The seahtrue ouput\nNow go and have a look at the run_seahtrue output.\nLoading\n  webR...\n\n\n  \n\n\nAlso pluck some of the data\nLoading\n  webR...\n\n\n  \n\n\nSome data are simple character strings, like the date column, whereas others are large tables like the raw_data column\nWith this loaded data (seahtrue_output_donor_A) you can now do similar plotting as in the plotting seahorse chapter. For this we only have to pluck the rate_data out of the data set. Be carefull that we preprocessed the data and we have other column names now so first glimpse the data.\nLoading\n  webR...\n\n\n  \n\n\nYou will see that the column names are labeled with wave, in this way we can distinguish for example the time column in the raw_data tibble from the time_wave column in the rate_data tibble. Also, please notice that we have OCR_wave_bc and OCR_wave. This distinctino is made because we can have OCR data that is background corrected or not. When clicking on the background slider in the Wave software from Agilent, the OCR data will be changed to non background corrected. If at this point the data is exported the xlsx input file is not background corrected. In the seahtrue this will show up as OCR_wave. Typically however the data is background corrected, so we most of the time have OCR_wave_bc.\n\n\n\n\n\n\ntime and time again\n\n\n\n\n\nSince rate is an aggregate of mulitple O2 or pH readings, also the definition of the timing of each measurement is different between the rate_data and the raw_data. Therefore in the seahtrue package both times are labeled differently. For the rate_table we labeled it with time_wave and for the raw_data we labeled it with timescale. And again, we used timescale to distinguish it from the time in the original input file.\n\n\n\nPlease note if we want to plot the OCR vs time, we have to use the OCR_wave_bc vs time_wave in our ggplot aesthetics.\nIt is good practice to have a quick look at how the groups were named in the experiment. We can use the pull(group) and unique() commands for this:\nLoading\n  webR...\n\n\n  \n\n\nNext, take some of the groups and plot them in a ggplot:\nLoading\n  webR...\n\n\n  \n\n\nGreat, this looks exactly the same as the plot we generated using the data from the downloaded excel file in the “plotting seahorse” chapter."
  },
  {
    "objectID": "swim_summary.html#what-did-we-learn",
    "href": "swim_summary.html#what-did-we-learn",
    "title": "8  Summary",
    "section": "8.1 What did we learn?",
    "text": "8.1 What did we learn?\n\n8.1.1 Functions\n\nDefinitions of functions\nFunction argument\nHow to build and execute functions\nHow to use the dplyr mutate function\nSome basic understanding of the seahtrue read_xfplate functions\nSome of the seahtrue preprocess_xfplate functions\nThe run_seahtrue function\n\n\n\n8.1.2 Outputs\n\nWhat nested tibbles are, and why they are usefull\nHow to generate nested tibbles\nHow to use the purrr map function on nested tibbles\nGet familiar with the semantics of the map function\nThe difference between .by and group_by\nHow to isolate or pluck data from a larger dataset\nGet familiar with the seahtrue output nested tibble\nThe different parameters for time in a seahorse dataset\nHow to access the data in seahtrue output and use it in ggplot"
  },
  {
    "objectID": "swim_summary.html#what-we-did-not-learn",
    "href": "swim_summary.html#what-we-did-not-learn",
    "title": "8  Summary",
    "section": "8.2 What we did not learn?",
    "text": "8.2 What we did not learn?\n\nHow to run seahtrue on our own data\nHow all the assertions and input checking in the seahtrue package are implemented\nMore elaborate use cases of the map function from the purrr package, like map2 or map_dbl"
  },
  {
    "objectID": "dive.html",
    "href": "dive.html",
    "title": "Diving deeper",
    "section": "",
    "text": "In the previous chapters we learned R and got familiar with the seahtrue package and its output. Here we will explore what is possible with the seahtrue data in R. We will go through making some nice visualisations of the raw data and rate data. Also we will show how to combine multiple experiments into one tibble and visualize the data."
  },
  {
    "objectID": "dive_alone.html#plate-map",
    "href": "dive_alone.html#plate-map",
    "title": "9  Single experiment",
    "section": "9.1 Plate map",
    "text": "9.1 Plate map\nTo dive a bit deeper into a single seahtrue experiment, we will first generate an overview of what the experimental set-up was.\nLet’s load the data first\nLoading\n  webR...\n\n\n  \n\n\nNext, we make a theme that we can use for the heatmap\nLoading\n  webR...\n\n\n  \n\n\nThen we make a nice default heatmap with the geom_tile function.\nLoading\n  webR...\n\n\n  \n\n\nThe default ggplot colors are quite colorfull, but might hurt your eyes… If we want colors that are different than the default ggplot colors, and we want the legend to be nicely in order we need to add some additional code.\nLoading\n  webR...\n\n\n  \n\n\nAnother option would be to manually arrange the factors in a way that suits you best.\nLoading\n  webR..."
  },
  {
    "objectID": "dive_alone.html#background",
    "href": "dive_alone.html#background",
    "title": "9  Single experiment",
    "section": "9.2 Background",
    "text": "9.2 Background\nIn Seahorse experiments the corners of the plate are by default assigned as Background wells, meaning that in these wells there is no sample but does have the same conditions and culture medium as your sample wells. Background wells need to be checked for outliers. This is not obvious from the Wave software interface, because the backgroung is by default substracted and users will never see the actual background data, unless they really select for it in the point-and-click software Wave. So let’s make some plots of the raw background O2 data.\nWe will now use the raw_data table for plotting, and we assume you allready loaded the data file above in this session.\nLoading\n  webR...\n\n\n  \n\n\nThis is a nice plot of the background O2 readings. It does look weird, especailly beause there is one well H01 which has a comppletely different trend then the other wells. This might be suspected as a technical outlier. Possibly in this well there was not enough culture medium or the sensor was damaged. The lab details and observations should be aligned with the outlier calling to make sure to not erroneously flag a well as an outlier.\nTo make an even better visual representation of the background and to account for the different aspects how the background well data behaves we can plot only the first ticks of each measurement. We will also shift here now to the fluorescence readings of the Seahorse. Since the O2 is derived from fluorescence values in our experiments it would be good to really look at the most raw data that we get out of our experiment. The fluorescence is given as the parameter O2_em_corr\nWe have a plotting function that automates this.\nLoading\n  webR...\n\n\n  \n\n\nWe can use this function when we provide the right arguments. The argument option for the var are: O2_em_corr, pH_em_corr and O2_mmHg.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nTo calculate O2 from emission, Seahorse uses the Stern-Volmer equation. Find out (using google or chatGPT) what the stern-volmer equation is and write it in the form of a function. Use the arguments x, KSV, and F0.\nYou can also use the Gerenscer et al. paper that describes the calculations. The method and algorithms described in this Analytical Chemistry paper from 2009 are still used today. Gerencser et al. Anal Chem 2009\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 1\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nWhere x is the emission (O2_em_corr), KSV is a constant, the stern-volmer konstant, and F0 is the emission at zero oxygen. The values of these two constants is are unique to the cartridge that you used during your experiment. Seahorse provides these numbers when updating your Wave software and matches them via a barcode read on the cartridge each run.\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nThe KSV and F0 are provided in the assay configuration sheet of the excel output. Seahtrue puts that information in the assay_info table. You can access it using the pluck function. In this case you have to use pluck two times, first to get to the assay_info and next to the KSV or F0\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 2\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nNow use the two constants KSV and F0, and the function stern_volmer to calculate the O2 from O2_em_corr. Also use select(well, measurement, tick, O2_mmHg, O2) to compare the O2 with the O2_mmHg in the output.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 3\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nPlot the O2 background values that you just calculated using the plot_raw_BKGD function. Compare the plot to when plotting the O2_mmHg that was derived from the Seahorse output xlsx.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 4\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nApparently the O2_mmHg is different from our own calculated O2 concentrations. When looking at the O2_mmHg background plot it looks like that these O2 values are also corrected for a background. Let’s see if that is indeed the case.\nSeahorse Wave substracts the mean background from all samples. So the mean O2_mmHg of the “Background group is substracted from all samples wells (and background wells apparently). We can also do that with our seahtrue data. We should take care of what we need to summarize here, each tick is a unique measurement in the raw_data, do let’s take tick as the .by parameter\nLoading\n  webR...\n\n\n  \n\n\nNow we need to substract the background O2 from all other wells and the backgrounds wells themselves.\nThe way this is done in the Seahorse algorithm is to take into account the ambient O2 levels. Basically what Seahorse calculates is the following:\nLoading\n  webR...\n\n\n  \n\n\nNow compare the O2_corrected with the original O2_mmHg. Do this in two ways. 1) make a ggplot with the O2_corrected on x-axis and O2_mmHg on the y-axis. and 2) use the plot_raw_BKGD function with O2_corrected and compare with the output from the O2_mmHg plot_raw_BKGD plot\n\n\n\n\n\n\n\n\nSolution to Exercise 5\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nLoading\n  webR...\n\n\n  \n\n\nAlthough the values are not identical, they are pretty close. Indicating that the O2_mmHg background data is likely also corrected for background in this dataset."
  },
  {
    "objectID": "dive_alone.html#low-signals",
    "href": "dive_alone.html#low-signals",
    "title": "9  Single experiment",
    "section": "9.3 Low signals",
    "text": "9.3 Low signals\nSometimes we don’t have much sample. In most cases the sample is cells, and with low cell number the O2 consumption and extracellular acidification can be low. Seahorse defines an pretty arbitrary cut-off for basal respiration at 20 pmol/min. Below this value OCR becomes less reliable.\nIn the loaded experiment seahtrue_output_donor_A, we have a group labeled with 50.000. In these wells we only have 50.000 cells in each well, which makes its signal difficult to detect.\nLoading\n  webR...\n\n\n  \n\n\nPlease notice that the OCR signal for the 50.000 group is definitely below 20 pmol/min.\nWhat if we want to investigate in more detail how our signals are for our samples with this low respiration?\nWe can make use of the raw_data again and plot the background O2 signal with the sample O2 signal in one plot. Since in the previous section we saw that O2 signals for the background wells were also corrected for background (?!), we will work with our own calculated O2 levels using the stern_volmer function we wrote in the previous section.\nAlso, we will use quite a big plotting function for this. It offers some flexibility on whether we want to plot means and/or scale the data. Also we can select specific wells and which measurements.\nLoading\n  webR...\n\n\n  \n\n\nLet’s explore this huge function (with not so tidy coding in it….), by using it:\nLoading\n  webR...\n\n\n  \n\n\nYou can see in this plot that background O2 levels rise in each measurment. This drift is consistently seen in all instruments and experimental condtions. The upward drift in O2, is also why OCRs for background wells are often negative in your Seahorse software Wave graphs (when you point-and-click to have the background not substracted). The explanation that Gerenscer et al. gave for the drift was that either 1) temperature is not stable during a measurement and the fluorescent sensors are temperature sensitive or 2) that O2 levels in the microchamber that is formed when probe is at its measuring position is entering from the plastic or culture medium above. Both reasons are debatable I think.\nAlthough the O2 levels of backgrounds increase, it can be seen that the O2 levels of well D02 increase less. Meaning that there oxygen consumption is higher than the background.\n\n\n\n\n\n\nExercise 6\n\n\n\nChange the inputs for the plot_raw_whichGroup_dots (in a meaningful way), to plot the 1) O2_em_corr, 2) plot another well D04 (please note that you also have change the group name because it is from the 100.000 group)\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 6\n\n\n\n\n\nLoading\n  webR..."
  },
  {
    "objectID": "dive_alone.html#plotting-basal-and-maximal-respiration",
    "href": "dive_alone.html#plotting-basal-and-maximal-respiration",
    "title": "9  Single experiment",
    "section": "9.4 Plotting basal and maximal respiration",
    "text": "9.4 Plotting basal and maximal respiration\nPluck the injection_info table from the seahtrue_output_donor_A dataset to see what how the injections were defined in the experimental set-up before running the seahorse.\nLoading\n  webR...\n\n\n  \n\n\nThis is not a typical mito-stress test experiment, where we inject oligomycin, FCCP and antimycinA/rotenone sequentially. Instead we inject only FCCP and antimycinA/rotenone.\nTo get the maximal and basal respiration out of the ocr rate table, we need to do some calculations. We first make some assumptions and defintions:\n\nWe call each interval between two injections or between start and an injection or between an injection and the end a phase\nEach phase has a unique name that is named after the injection that was last. The first phase after the start is called init_ocr and we also typically have the phases om_ocr, fccp_ocr and amrot_ocr. Phases are marked with either _ocr or _ecar, because these are distinct parameters.\nTo calculate respiration parameters, like basal respiration (= basal_ocr), we define the following:\n\nbasal_ocr = init_ocr - amrot_ocr.\nmax_ocr = fccp_ocr - amrot_ocr\nspare_ocr = fccp_ocr - init_ocr\nproton_leak = om_ocr = amrot_ocr\natp_linked = init_ocr - om_ocr\n\nWe also use indices to have relative parameters:\n\nspare_ocr_index = (spare_ocr / basal_ocr)*100\nbasal_ocr_index = (basal_ocr / max_ocr)*100\nleak_index = (proton_leak / basal_ocr)*100\ncoupling_index = (atp_linked / basal_ocr)*100) %>%\n\nAnother important assumption is that we are not using average values to represent each phase, but intead we use a specific measurement. The reason for this is that we assume that for all phases, except FCCP, three measurements are needed in time to get to steady-state. For FCCP injection, we assume that it reaches steady-state fast, or at least its maximal ocr, so we take the first measurement after injection as the measurement representing the FCCP phase.\n\nLet’s now put that into R code. We call the type of experiment we did in this dataset a maximal capacity (maxcap) test.\nWe also injected monensin, which can maximize ECAR, but we don’t need it for OCR calculations.\nLoading\n  webR...\n\n\n  \n\n\nNow for each well we have the parameters related to the phases that we defined in the parameter set.\nNext we want to calculate the respiration parameters and indices.\nLoading\n  webR...\n\n\n  \n\n\nWith this data we can plot our typical basal and maximal bar/scatter plots that we see in our lovely papers, presentations and theses.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nThe plot above shows maximal ocr. Now make your own plot with 1) basal_ocr and 2) all groups except background. Make sure to order the group legend tidyly and have reable x-axis labels.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 7\n\n\n\n\n\nLoading\n  webR..."
  },
  {
    "objectID": "dive_together.html#gimme-more-plates",
    "href": "dive_together.html#gimme-more-plates",
    "title": "10  Multiple experiments",
    "section": "10.1 Gimme more plates",
    "text": "10.1 Gimme more plates\nThe reading and processing that seahtrue uses, together with the nested tibble output, has another benefit. It allows for collecting and organizing multiple experiments into one nested tibble. We can do that by loading multiple xlsx files into our working directory, making a file list of those excel file names and mapping the run_seahtrue function against the file list.\nFor now we will first have a look at how the output looks like. We load the three experiment output from github.\nLoading\n  webR...\n\n\n  \n\n\nYou can see we have one experiment per row of the tibble . They are labeled with the plate_id, file_path, and date in the first three columns. Since it is in one tibble, we can now pluck for example all raw_data from the complete tibble:\nLoading\n  webR...\n\n\n  \n\n\nYou see we now have a tibble of almost 2 million rows and 22 columns, nicely and tidyly loaded in our webR/wasm R environment!! How cool is that! Our favorite point-and-click software Excel wouldn’t even be able to handle this amount of rows.\nNow we can plot for example the distribution of emission values for each experiment. We use the ggridges package for this.\nLoading\n  webR...\n\n\n  \n\n\nWith only a couple of lines of code we go from raw_data to real insights in your data. For example,\n\nThe three experiments seem to have similar distirbutions between groups, but the third experiment V01744.... looks a bit different than the first two\nFrom the plot, it can be seen that in the group with highest cell density 300.000 the O2 levels are still in a range that the wells do not become hypoxic. The lowest O2 levels are at around 100 mmHg.\nFor the 50.000 group in the first two experiments there seems to be some signal in the right tail of the distribution which is not obviously in the third experiment 50.000 group or in the other groups.\n\nLet’s explore this right tail of the 50.000 group in more detail.\nLoading\n  webR...\n\n\n  \n\n\nSo what are these high O2_mmHg values. We can find out by looking at the data for the individuals wells. By filtering for the first plate we can see the O2_mmHg in more detail. By using a short unique string as input for the str_detect we only have to type a couple of characters, which saves us some time. Now we make a simple scatter plot.\nLoading\n  webR...\n\n\n  \n\n\nIndeed, there are two wells that have high O2_mmHg. Because of the many colors it is difficult to see which wells they are. Now do some ggplot trickery to label the two wells with higher O2.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nMake the same scatter plot for the other two plates for the 50.000 group, and observe if there are wells with abnormally high O2.\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 1\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\nLoading\n  webR..."
  },
  {
    "objectID": "surf_seahtrue.html#code",
    "href": "surf_seahtrue.html#code",
    "title": "11  Running seahtrue functions",
    "section": "11.1 code",
    "text": "11.1 code\nYou can click on the Run code to get all functions into the memory of your session. Since R uses lazy evaluation what happens is that just a bunch a text is assigned to the function name and there is no evaluation of the code. That also means that it is quickly run. I have a small text output to see that indeed the code has been run.\n\n11.1.1 read_xf_plate()\nLoading\n  webR...\n\n\n  \n\n\n\n\n11.1.2 preprocess_xfplate()\nLoading\n  webR...\n\n\n  \n\n\n\n\n11.1.3 run_seahtrue()\nLoading\n  webR..."
  },
  {
    "objectID": "surf_seahtrue.html#run-it",
    "href": "surf_seahtrue.html#run-it",
    "title": "11  Running seahtrue functions",
    "section": "11.2 run it",
    "text": "11.2 run it\nFor one file\nLoading\n  webR...\n\n\n  \n\n\nFor three files\nLoading\n  webR...\n\n\n  \n\n\nHere we use the nice map function from purrr that we introduced previously:\nLoading\n  webR...\n\n\n  \n\n\n… and we plot the time vs O2 for the raw data for the second experiment\nLoading\n  webR..."
  },
  {
    "objectID": "surf_seahtrue.html#bioenergetic-space-plot",
    "href": "surf_seahtrue.html#bioenergetic-space-plot",
    "title": "11  Running seahtrue functions",
    "section": "11.3 Bioenergetic space plot",
    "text": "11.3 Bioenergetic space plot\nFor making the bioenergetic space plots that were proposed in the Mookerjee et al. JBC Quantifying intracellular rates of glycolytic and oxidative ATP production and consumption using extracellular flux measurements, we need to do four things:\n\nEstablish buffering power of the culture medium\nCorrect ECAR for OCR-derived acidification\nGet the ATP assumptions and formulas into R\nPlot the ATP fluxes in a space plot and derive the indices\n\nThe first needs to be done in the lab using established protocols for calculating buffering power, preferably with H2SO4 as acid. The second to fourth are merely some data analysis and plotting in R.\n\n11.3.1 Buffering power\nWe assume you allready know what the buffering power is of your particular culture medium and assigned that to the meta info of your experiment before running your seahorse plate. In that case it is listed in the bufferfactor column in our raw_data output data. Since each well can theoretically have a different type of culturing medium and thus also a different buffering factor, each well gets its own buffer factor\nLet’s have a look at the buffering power numbers in the xf_3_read data.\nLoading\n  webR...\n\n\n  \n\n\nYou will see that if there is 0and 2.4. Likely, the background wells were assigned zero and the sample wells 2.4.\nLoading\n  webR...\n\n\n  \n\n\nThat is indeed the case, as you can see if you run the above code.\n\n\n11.3.2 Calculations\n\n11.3.2.1 OCR-derived ECAR\nWe can calculate the contribution of OCR-derived H+ in multiple ways. Agilent takes an emprical approach and in the Mookerjee paper it is based on enzyme knetics and steady-state assumptions. For the agilent approach the contribution was empiraclly determined as 0.61 which was named the CCF (C02 contribution factor). For the mookerjee method the factor was 0.953.\n\n\n11.3.2.2 ATP per O2\nFor the agilent approach they take the P/O ratio as 2.75, which was assumes and empirically tested. For the mookerjee approach, the P/O ratio was assumed to be 2.611, also with a hyperpolarization factor included.\n\n\n11.3.2.3 ATP per lactate\nAgilent assumed the ATP/lactate ratio is 1. Although Mookerjee is doing the same, they also take into account that for each each lactate derived from glucose there can be O2 consumption that produced ATP, this is a factor of 0.242.\n\n\n11.3.2.4 Combined\nSince these are just transformations of our OCR and ECAR data, we can combine all these factors into three separate factors:\n\nJglyco_ecar_factor\nJglyco_ocr_factor\nJoxphos_ocr_factor\n\nWe will also include a scaling factor for data that is not normalized to protein level or cell number. This scalling factor brings the data into the same range as the Mookerjee paper. The estimation of protein concentration in a seahorse well is 10-30 ug per 100000 cells. We thus take 20 ug as an estimate of protein amount in a well.\n\n\n11.3.2.5 Code\nHere is a function for calculating and a pipe for plotting the space. The first is calculate_space, it takes in the following arguments:\n\nrate => rate_data format\nparam_set_ocr => timepoints for phases ocr\nparam_set_ecar => timepoints for phases ecar\natp_factors => atp calculation factors\nug_protein_scaling_factor => scaling factor\nOCR_var => either OCR_wave_bc or J_oxpos\nECAR_var => either ECAR_wave_bc or J_glyco\n\nLoading\n  webR...\n\n\n  \n\n\n\n\n\n11.3.3 Space plot\nWe set the arguments first for plotting the biospace\nLoading\n  webR...\n\n\n  \n\n\nNext we set the input rate table argument\nLoading\n  webR...\n\n\n  \n\n\nRun the space function\nLoading\n  webR...\n\n\n  \n\n\nPlot the df_space\nLoading\n  webR...\n\n\n  \n\n\n\n\n11.3.4 Gauge plots\nLoading\n  webR...\n\n\n  \n\n\nLoading\n  webR..."
  }
]